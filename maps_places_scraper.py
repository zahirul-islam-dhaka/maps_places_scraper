# -*- coding: utf-8 -*-
"""maps_places_scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iTrqdnYhgrOqAOqoxn8UmUV-GfdQWGvf
"""

# Google Maps Places Scraper (Async Playwright)
#
# Overview:
# This notebook extracts publicly available business information from Google Maps
# search results using Playwright (async). It scrolls through listings, opens
# individual place details, and collects structured data for analysis or reporting.
#
# What this notebook does:
# - Searches Google Maps using a keyword (e.g., "Bar in Dhaka")
# - Automatically scrolls the results panel
# - Opens each business listing
# - Extracts key business details
# - Saves results to a CSV file
#
# Data fields collected:
# - Business name
# - Address
# - Website
# - Phone number
# - Rating and review count
# - Opening time
# - Store shopping, pickup, and delivery availability
# - Business type and short description
#
# Typical use cases:
# - Market and competitor analysis
# - Business directory creation
# - Location-based research
# - Academic and policy research using public data
#
# Environment:
# - Google Colab
# - Python 3.9+
# - Playwright (Chromium)
# - pandas
# - asyncio
#
# Responsible use:
# This notebook collects publicly visible information only.
# Users are responsible for complying with Google Maps Terms of Service
# and applicable laws.
#
# Author:
# Zahirul Islam
# Senior Project Associate, ARCED Foundation
#
# Contact:
# M: +880 1688 831919
# W: https://www.arced.foundation
# E: zahirul.islam@arced.foundation

import logging
from typing import List, Optional
from playwright.async_api import async_playwright, Page
from dataclasses import dataclass, asdict
import pandas as pd
import argparse
import platform
import time
import os
import asyncio

@dataclass
class Place:
    name: str = ""
    address: str = ""
    Open: str = ""
    Amount: str = ""
    website: str = ""
    phone_number: str = ""
    reviews_count: Optional[int] = None
    reviews_average: Optional[float] = None
    store_shopping: str = ""
    in_store_pickup: str = ""
    store_delivery: str = ""
    place_type: str = ""
    opens_at: str = ""
    introduction: str = ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

async def extract_text(page: Page, xpath: str) -> str:
    try:
        if await page.locator(xpath).count() > 0:
            return await page.locator(xpath).inner_text()
    except Exception as e:
        logging.warning(f"Failed to extract text for xpath {xpath}: {e}")
    return ""

async def extract_place(page: Page) -> Place:
    # XPaths
    name_xpath = '//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]'
    address_xpath = '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'
    website_xpath = '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'
    phone_number_xpath = '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'
    reviews_count_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]//div//span//span//span[@aria-label]'
    reviews_average_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]//div//span[@aria-hidden]'
    info1 = '//div[@class="LTs0Rc"][1]'
    info2 = '//div[@class="LTs0Rc"][2]'
    info3 = '//div[@class="LTs0Rc"][3]'
    opens_at_xpath = '//button[contains(@data-item-id, "oh")]//div[contains(@class, "fontBodyMedium")]'
    opens_at_xpath2 = '//div[@class="MkV9"]//span[@class="ZDu9vd"]//span[2]'
    place_type_xpath = '//div[@class="LBgpqf"]//button[@class="DkEaL "]'
    intro_xpath = '//div[@class="WeS02d fontBodyMedium"]//div[@class="PYvSYb "]'

    place = Place()
    place.name = await extract_text(page, name_xpath)
    place.address = await extract_text(page, address_xpath)
    place.website = await extract_text(page, website_xpath)
    place.phone_number = await extract_text(page, phone_number_xpath)
    place.place_type = await extract_text(page, place_type_xpath)
    place.introduction = await extract_text(page, intro_xpath) or "None Found"

    # Reviews Count
    reviews_count_raw = await extract_text(page, reviews_count_xpath)
    if reviews_count_raw:
        try:
            temp = reviews_count_raw.replace('\xa0', '').replace('(','').replace(')','').replace(',','')
            place.reviews_count = int(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews count: {e}")
    # Reviews Average
    reviews_avg_raw = await extract_text(page, reviews_average_xpath)
    if reviews_avg_raw:
        try:
            temp = reviews_avg_raw.replace(' ','').replace(',','.')
            place.reviews_average = float(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews average: {e}")
    # Store Info
    for idx, info_xpath in enumerate([info1, info2, info3]):
        info_raw = await extract_text(page, info_xpath)
        if info_raw:
            temp = info_raw.split('·')
            if len(temp) > 1:
                check = temp[1].replace("\n", "").lower()
                if 'shop' in check:
                    place.store_shopping = "Yes"
                if 'pickup' in check:
                    place.in_store_pickup = "Yes"
                if 'delivery' in check:
                    place.store_delivery = "Yes"
    # Opens At
    opens_at_raw = await extract_text(page, opens_at_xpath)
    if opens_at_raw:
        opens = opens_at_raw.split('⋅')
        if len(opens) > 1:
            place.opens_at = opens[1].replace("\u202f","")
        else:
            place.opens_at = opens_at_raw.replace("\u202f","")
    else:
        opens_at2_raw = await extract_text(page, opens_at_xpath2)
        if opens_at2_raw:
            opens = opens_at2_raw.split('⋅')
            if len(opens) > 1:
                place.opens_at = opens[1].replace("\u202f","")
            else:
                place.opens_at = opens_at2_raw.replace("\u202f","")
    return place

async def scrape_places(search_for: str, total: int) -> List[Place]:
    setup_logging()
    places: List[Place] = []
    async with async_playwright() as p:
        if platform.system() == "Windows":
            browser_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            browser = await p.chromium.launch(executable_path=browser_path, headless=True)
        else:
            browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto("https://www.google.com/maps/@32.9817464,70.1930781,3.67z?", timeout=60000)
            await page.wait_for_timeout(1000)

            # Using get_by_role for more robust search box identification
            search_box = page.get_by_role("combobox", name="Search Google Maps")
            await search_box.wait_for(timeout=30000)
            await search_box.fill(search_for)
            await page.keyboard.press("Enter")

            await page.wait_for_timeout(5000)

            # Find the scrollable results pane
            scroll_pane_xpath = '//div[@role="feed"]'
            await page.wait_for_selector(scroll_pane_xpath, timeout=30000)
            scroll_pane = page.locator(scroll_pane_xpath)

            # Locator for individual result cards within the scroll pane
            # div[role="article"] is a common and robust selector for business listings
            listing_card_locator = scroll_pane.locator('div[role="article"]')

            previously_counted = 0
            # Scroll until 'total' items are found or no more items load
            while True:
                await scroll_pane.evaluate("node => node.scrollTop = node.scrollHeight")
                await page.wait_for_timeout(2000)

                current_found = await listing_card_locator.count()
                logging.info(f"Currently Found: {current_found}")

                if current_found >= total:
                    logging.info(f"Reached target total of {total} listings.")
                    break
                if current_found == previously_counted:
                    logging.info("Arrived at all available listings.")
                    break
                previously_counted = current_found

            # Get all the actual listing locators after scrolling
            all_listings = await listing_card_locator.all()
            listings_to_process = all_listings[:total]

            logging.info(f"Total Found for processing: {len(listings_to_process)}")

            for idx, listing in enumerate(listings_to_process):
                try:
                    await listing.click()
                    # Increased timeout for waiting for the name element on the detail page
                    await page.wait_for_selector('//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]', timeout=10000)
                    time.sleep(1.5)
                    place = await extract_place(page)
                    if place.name:
                        places.append(place)
                    else:
                        logging.warning(f"No name found for listing {idx+1}, skipping.")
                except Exception as e:
                    logging.warning(f"Failed to extract listing {idx+1}: {e}")
        finally:
            await browser.close()
    return places

def save_places_to_csv(places: List[Place], output_path: str = "result.csv", append: bool = False):
    df = pd.DataFrame([asdict(place) for place in places])
    if not df.empty:
        for column in df.columns:
            if df[column].nunique() == 1:
                df.drop(column, axis=1, inplace=True)
        file_exists = os.path.isfile(output_path)
        mode = "a" if append else "w"
        header = not (append and file_exists)
        df.to_csv(output_path, index=False, mode=mode, header=header)
        logging.info(f"Saved {len(df)} places to {output_path} (append={append})")
    else:
        logging.warning("No data to save. DataFrame is empty.")

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", type=str, help="Search query for Google Maps")
    parser.add_argument("-t", "--total", type=int, help="Total number of results to scrape")
    parser.add_argument("-o", "--output", type=str, default="result.csv", help="Output CSV file path")
    parser.add_argument("--append", action="store_true", help="Append results to the output file instead of overwriting")
    args = parser.parse_args([])
    search_for = args.search or "Bar in Dhaka"
    total = args.total or 200 # Changed total to 200
    output_path = args.output
    append = args.append
    places = await scrape_places(search_for, total)
    save_places_to_csv(places, output_path, append=append)

if __name__ == "__main__":
    # Run the async main function
    import nest_asyncio
    nest_asyncio.apply()
    asyncio.run(main())

"""# Task
Analyze the scraped data from "result.csv" by loading it into a pandas DataFrame, cleaning and preparing relevant columns such as 'reviews_count' and 'reviews_average', and then performing a comprehensive analysis of place types, review statistics, and store services. Finally, visualize key metrics and provide a summary of the findings.

## Load Scraped Data

### Subtask:
Load the scraped data from 'result.csv' into a pandas DataFrame. Inspect the first few rows, columns, and data types.

**Reasoning**:
To load the scraped data from 'result.csv' into a pandas DataFrame and inspect its structure, I need to import pandas, read the CSV file, display the first few rows, and print a concise summary of the DataFrame.
"""

import pandas as pd

df = pd.read_csv('result.csv')
print("First 5 rows of the DataFrame:")
print(df.head())
print("\nDataFrame Info:")
df.info()

"""## Re-run Data Scraping

### Subtask:
Re-run the data scraping process to generate a populated 'result.csv' file for analysis.

**Reasoning**:
The previous scraping attempt resulted in an empty or malformed `result.csv`. To properly generate a populated 'result.csv' for analysis, I will directly call the `scrape_places` and `save_places_to_csv` functions with the `SEARCH_KEYWORD` and `MAX_PLACES` variables available in the kernel state. This approach bypasses the `argparse` setup in the `main` function, ensuring the correct parameters are used for scraping.
"""

import asyncio
import nest_asyncio
import pandas as pd
import os

# Ensure async operations work in a notebook environment
nest_asyncio.apply()

# Use the variables from the kernel state for search query and total results
search_query = SEARCH_KEYWORD
total_results = MAX_PLACES
output_file = "result.csv"
append_mode = False # Overwrite previous (potentially empty) result.csv

print(f"Starting data scraping for '{search_query}' (total: {total_results})...")

try:
    # Call the async scraping function directly
    scraped_places = asyncio.run(scrape_places(search_query, total_results))

    # Save the scraped data to CSV
    save_places_to_csv(scraped_places, output_file, append=append_mode)

    print(f"Scraping completed. Saved {len(scraped_places)} places to '{output_file}'.")

    # Verify the content of the newly created CSV
    df_new = pd.read_csv(output_file)
    print("\nFirst 5 rows of the newly scraped DataFrame:")
    print(df_new.head())
    print("\nDataFrame Info after re-scraping:")
    df_new.info()

except NameError as e:
    print(f"Error: One or more required functions (e.g., scrape_places, save_places_to_csv) are not defined. Please ensure the initial scraping code cell (nmQp6tE-PNSs) has been executed: {e}")
except Exception as e:
    print(f"An unexpected error occurred during scraping: {e}")

"""**Reasoning**:
The previous scraping attempt resulted in an empty `reviews_count` column, indicating an issue with its XPath or parsing logic. Additionally, a timeout error during page navigation suggests some instability. I will refine the `reviews_count_xpath` to be more robust, implement regex for parsing the count, and increase the timeout for waiting on the place details page to improve reliability.


"""

import logging
from typing import List, Optional
from playwright.async_api import async_playwright, Page
from dataclasses import dataclass, asdict
import pandas as pd
import argparse
import platform
import time
import os
import asyncio
import re

@dataclass
class Place:
    name: str = ""
    address: str = ""
    website: str = ""
    phone_number: str = ""
    reviews_count: Optional[int] = None
    reviews_average: Optional[float] = None
    store_shopping: str = "No"
    in_store_pickup: str = "No"
    store_delivery: str = "No"
    place_type: str = ""
    opens_at: str = ""
    introduction: str = ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

async def extract_text(page: Page, xpath: str) -> str:
    try:
        if await page.locator(xpath).count() > 0:
            return await page.locator(xpath).inner_text()
    except Exception as e:
        logging.warning(f"Failed to extract text for xpath {xpath}: {e}")
    return ""

async def extract_place(page: Page) -> Place:
    # XPaths
    name_xpath = '//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]'
    address_xpath = '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'
    website_xpath = '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'
    phone_number_xpath = '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'

    # Common parent for reviews info
    reviews_info_parent_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]'
    reviews_average_xpath = f'{reviews_info_parent_xpath}//span[@aria-hidden]'
    # Revised reviews_count_xpath to be more robust, looking for span with "review" in aria-label or parentheses in text
    reviews_count_xpath = f'{reviews_info_parent_xpath}//span[contains(@aria-label, "review")] | {reviews_info_parent_xpath}//span[contains(text(), "(") and contains(text(), ")")]'

    info1 = '//div[@class="LTs0Rc"][1]'
    info2 = '//div[@class="LTs0Rc"][2]'
    info3 = '//div[@class="LTs0Rc"][3]'
    opens_at_xpath = '//button[contains(@data-item-id, "oh")]//div[contains(@class, "fontBodyMedium")]'
    opens_at_xpath2 = '//div[@class="MkV9"]//span[@class="ZDu9vd"]//span[2]'
    place_type_xpath = '//div[@class="LBgpqf"]//button[@class="DkEaL "]'
    intro_xpath = '//div[@class="WeS02d fontBodyMedium"]//div[@class="PYvSYb "]'

    place = Place()
    place.name = await extract_text(page, name_xpath)
    place.address = await extract_text(page, address_xpath)
    place.website = await extract_text(page, website_xpath)
    place.phone_number = await extract_text(page, phone_number_xpath)
    place.place_type = await extract_text(page, place_type_xpath)
    place.introduction = await extract_text(page, intro_xpath) or "None Found"

    # Reviews Count
    reviews_count_raw = await extract_text(page, reviews_count_xpath)
    if reviews_count_raw:
        try:
            # Use regex to extract numbers, ignoring parentheses or "reviews" text
            temp = re.search(r'\d{1,3}(?:,\d{3})*', reviews_count_raw)
            if temp:
                place.reviews_count = int(temp.group(0).replace(',', ''))
            else:
                logging.warning(f"Failed to find number in reviews count string: '{reviews_count_raw}'")
        except Exception as e:
            logging.warning(f"Failed to parse reviews count from '{reviews_count_raw}': {e}")
    # Reviews Average
    reviews_avg_raw = await extract_text(page, reviews_average_xpath)
    if reviews_avg_raw:
        try:
            temp = reviews_avg_raw.replace(' ','').replace(',','.')
            place.reviews_average = float(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews average from '{reviews_avg_raw}': {e}")

    # Store Info
    for idx, info_xpath in enumerate([info1, info2, info3]):
        info_raw = await extract_text(page, info_xpath)
        if info_raw:
            temp = info_raw.split('·')
            if len(temp) > 1:
                check = temp[1].replace("\n", "").lower()
                if 'shop' in check:
                    place.store_shopping = "Yes"
                if 'pickup' in check:
                    place.in_store_pickup = "Yes"
                if 'delivery' in check:
                    place.store_delivery = "Yes"
    # Opens At
    opens_at_raw = await extract_text(page, opens_at_xpath)
    if opens_at_raw:
        opens = opens_at_raw.split('⋅')
        if len(opens) > 1:
            place.opens_at = opens[1].replace("\u202f","")
        else:
            place.opens_at = opens_at_raw.replace("\u202f","")
    else:
        opens_at2_raw = await extract_text(page, opens_at_xpath2)
        if opens_at2_raw:
            opens = opens_at2_raw.split('⋅')
            if len(opens) > 1:
                place.opens_at = opens[1].replace("\u202f","")
            else:
                place.opens_at = opens_at2_raw.replace("\u202f","")
    return place

async def scrape_places(search_for: str, total: int) -> List[Place]:
    setup_logging()
    places: List[Place] = []
    async with async_playwright() as p:
        if platform.system() == "Windows":
            browser_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            browser = await p.chromium.launch(executable_path=browser_path, headless=True)
        else:
            browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto("https://www.google.com/maps/@32.9817464,70.1930781,3.67z?", timeout=60000)
            await page.wait_for_timeout(1000)

            # Using get_by_role for more robust search box identification
            search_box = page.get_by_role("combobox", name="Search Google Maps")
            await search_box.wait_for(timeout=30000)
            await search_box.fill(search_for)
            await page.keyboard.press("Enter")

            await page.wait_for_timeout(5000)

            # Find the scrollable results pane
            scroll_pane_xpath = '//div[@role="feed"]'
            await page.wait_for_selector(scroll_pane_xpath, timeout=30000)
            scroll_pane = page.locator(scroll_pane_xpath)

            # Locator for individual result cards within the scroll pane
            listing_card_locator = scroll_pane.locator('div[role="article"]')

            previously_counted = 0
            # Scroll until 'total' items are found or no more items load
            while True:
                await scroll_pane.evaluate("node => node.scrollTop = node.scrollHeight")
                await page.wait_for_timeout(2000)

                current_found = await listing_card_locator.count()
                logging.info(f"Currently Found: {current_found}")

                if current_found >= total:
                    logging.info(f"Reached target total of {total} listings.")
                    break
                if current_found == previously_counted:
                    logging.info("Arrived at all available listings.")
                    break
                previously_counted = current_found

            # Get all the actual listing locators after scrolling
            all_listings = await listing_card_locator.all()
            listings_to_process = all_listings[:total]

            logging.info(f"Total Found for processing: {len(listings_to_process)}")

            for idx, listing in enumerate(listings_to_process):
                try:
                    await listing.click()
                    # Increased timeout for waiting for the name element on the detail page
                    await page.wait_for_selector(name_xpath, timeout=15000)
                    time.sleep(1.5)
                    place = await extract_place(page)
                    if place.name:
                        places.append(place)
                    else:
                        logging.warning(f"No name found for listing {idx+1} after click, skipping.")
                except Exception as e:
                    logging.warning(f"Failed to extract listing {idx+1}: {e}")
        finally:
            await browser.close()
    return places

def save_places_to_csv(places: List[Place], output_path: str = "result.csv", append: bool = False):
    df = pd.DataFrame([asdict(place) for place in places])
    if not df.empty:
        # Drop columns that are entirely empty after all extractions
        df.dropna(axis=1, how='all', inplace=True)

        file_exists = os.path.isfile(output_path)
        mode = "a" if append else "w"
        header = not (append and file_exists)
        df.to_csv(output_path, index=False, mode=mode, header=header)
        logging.info(f"Saved {len(df)} places to {output_path} (append={append})")
    else:
        logging.warning("No data to save. DataFrame is empty.")

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", type=str, help="Search query for Google Maps")
    parser.add_argument("-t", "--total", type=int, help="Total number of results to scrape")
    parser.add_argument("-o", "--output", type=str, default="result.csv", help="Output CSV file path")
    parser.add_argument("--append", action="store_true", help="Append results to the output file instead of overwriting")
    args = parser.parse_args([]) # Pass empty list to prevent argparse from trying to parse sys.argv

    # Use kernel variables if they exist, otherwise use default/argparse defaults
    search_for = globals().get('SEARCH_KEYWORD', args.search or "turkish stores in toronto Canada")
    total = globals().get('MAX_PLACES', args.total or 1)
    output_path = args.output
    append = args.append

    places = await scrape_places(search_for, total)
    save_places_to_csv(places, output_path, append=append)

if __name__ == "__main__":
    # Run the async main function
    import nest_asyncio
    nest_asyncio.apply()
    asyncio.run(main())
# from google.colab import files
# files.download('result_test.csv')

"""**Reasoning**:
The previous error 'name 'name_xpath' is not defined' occurred because XPath variables like `name_xpath` were defined within the `extract_place` function, making them inaccessible to the `scrape_places` function when `page.wait_for_selector` was called. To fix this, I will move all XPath definitions to the global scope of the script, ensuring they are available to both `extract_place` and `scrape_places`.


"""

import logging
from typing import List, Optional
from playwright.async_api import async_playwright, Page
from dataclasses import dataclass, asdict
import pandas as pd
import argparse
import platform
import time
import os
import asyncio
import re

@dataclass
class Place:
    name: str = ""
    address: str = ""
    website: str = ""
    phone_number: str = ""
    reviews_count: Optional[int] = None
    reviews_average: Optional[float] = None
    store_shopping: str = "No"
    in_store_pickup: str = "No"
    store_delivery: str = "No"
    place_type: str = ""
    opens_at: str = ""
    introduction: str = ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

# XPaths defined globally to be accessible everywhere
name_xpath = '//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]'
address_xpath = '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'
website_xpath = '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'
phone_number_xpath = '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'
# Common parent for reviews info
reviews_info_parent_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]'
reviews_average_xpath = f'{reviews_info_parent_xpath}//span[@aria-hidden]'
# Revised reviews_count_xpath to be more robust, looking for span with "review" in aria-label or parentheses in text
reviews_count_xpath = f'{reviews_info_parent_xpath}//span[contains(@aria-label, "review")] | {reviews_info_parent_xpath}//span[contains(text(), "(") and contains(text(), ")")]'

info1 = '//div[@class="LTs0Rc"][1]'
info2 = '//div[@class="LTs0Rc"][2]'
info3 = '//div[@class="LTs0Rc"][3]'
opens_at_xpath = '//button[contains(@data-item-id, "oh")]//div[contains(@class, "fontBodyMedium")]'
opens_at_xpath2 = '//div[@class="MkV9"]//span[@class="ZDu9vd"]//span[2]'
place_type_xpath = '//div[@class="LBgpqf"]//button[@class="DkEaL "]'
intro_xpath = '//div[@class="WeS02d fontBodyMedium"]//div[@class="PYvSYb "]'

async def extract_text(page: Page, xpath: str) -> str:
    try:
        if await page.locator(xpath).count() > 0:
            return await page.locator(xpath).inner_text()
    except Exception as e:
        logging.warning(f"Failed to extract text for xpath {xpath}: {e}")
    return ""

async def extract_place(page: Page) -> Place:
    # XPaths are now globally defined, no need to redefine them here
    place = Place()
    place.name = await extract_text(page, name_xpath)
    place.address = await extract_text(page, address_xpath)
    place.website = await extract_text(page, website_xpath)
    place.phone_number = await extract_text(page, phone_number_xpath)
    place.place_type = await extract_text(page, place_type_xpath)
    place.introduction = await extract_text(page, intro_xpath) or "None Found"

    # Reviews Count
    reviews_count_raw = await extract_text(page, reviews_count_xpath)
    if reviews_count_raw:
        try:
            # Use regex to extract numbers, ignoring parentheses or "reviews" text
            temp = re.search(r'\d{1,3}(?:,\d{3})*', reviews_count_raw)
            if temp:
                place.reviews_count = int(temp.group(0).replace(',', ''))
            else:
                logging.warning(f"Failed to find number in reviews count string: '{reviews_count_raw}'")
        except Exception as e:
            logging.warning(f"Failed to parse reviews count from '{reviews_count_raw}': {e}")
    # Reviews Average
    reviews_avg_raw = await extract_text(page, reviews_average_xpath)
    if reviews_avg_raw:
        try:
            temp = reviews_avg_raw.replace(' ','').replace(',','.')
            place.reviews_average = float(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews average from '{reviews_avg_raw}': {e}")

    # Store Info
    for info_xpath in [info1, info2, info3]: # Removed idx as it was unused
        info_raw = await extract_text(page, info_xpath)
        if info_raw:
            temp = info_raw.split('·')
            if len(temp) > 1:
                check = temp[1].replace("\n", "").lower()
                if 'shop' in check:
                    place.store_shopping = "Yes"
                if 'pickup' in check:
                    place.in_store_pickup = "Yes"
                if 'delivery' in check:
                    place.store_delivery = "Yes"
    # Opens At
    opens_at_raw = await extract_text(page, opens_at_xpath)
    if opens_at_raw:
        opens = opens_at_raw.split('⋅')
        if len(opens) > 1:
            place.opens_at = opens[1].replace("\u202f","")
        else:
            place.opens_at = opens_at_raw.replace("\u202f","")
    else:
        opens_at2_raw = await extract_text(page, opens_at_xpath2)
        if opens_at2_raw:
            opens = opens_at2_raw.split('⋅')
            if len(opens) > 1:
                place.opens_at = opens[1].replace("\u202f","")
            else:
                place.opens_at = opens_at2_raw.replace("\u202f","")
    return place

async def scrape_places(search_for: str, total: int) -> List[Place]:
    setup_logging()
    places: List[Place] = []
    async with async_playwright() as p:
        if platform.system() == "Windows":
            browser_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            browser = await p.chromium.launch(executable_path=browser_path, headless=True)
        else:
            browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto("https://www.google.com/maps/@32.9817464,70.1930781,3.67z?", timeout=60000)
            await page.wait_for_timeout(1000)

            # Using get_by_role for more robust search box identification
            search_box = page.get_by_role("combobox", name="Search Google Maps")
            await search_box.wait_for(timeout=30000)
            await search_box.fill(search_for)
            await page.keyboard.press("Enter")

            await page.wait_for_timeout(5000)

            # Find the scrollable results pane
            scroll_pane_xpath = '//div[@role="feed"]'
            await page.wait_for_selector(scroll_pane_xpath, timeout=30000)
            scroll_pane = page.locator(scroll_pane_xpath)

            # Locator for individual result cards within the scroll pane
            listing_card_locator = scroll_pane.locator('div[role="article"]')

            previously_counted = 0
            # Scroll until 'total' items are found or no more items load
            while True:
                await scroll_pane.evaluate("node => node.scrollTop = node.scrollHeight")
                await page.wait_for_timeout(2000)

                current_found = await listing_card_locator.count()
                logging.info(f"Currently Found: {current_found}")

                if current_found >= total:
                    logging.info(f"Reached target total of {total} listings.")
                    break
                if current_found == previously_counted:
                    logging.info("Arrived at all available listings.")
                    break
                previously_counted = current_found

            # Get all the actual listing locators after scrolling
            all_listings = await listing_card_locator.all()
            listings_to_process = all_listings[:total]

            logging.info(f"Total Found for processing: {len(listings_to_process)}")

            for idx, listing in enumerate(listings_to_process):
                try:
                    await listing.click()
                    # Increased timeout for waiting for the name element on the detail page
                    await page.wait_for_selector(name_xpath, timeout=15000) # name_xpath is now globally available
                    time.sleep(1.5)
                    place = await extract_place(page)
                    if place.name:
                        places.append(place)
                    else:
                        logging.warning(f"No name found for listing {idx+1} after click, skipping.")
                except Exception as e:
                    logging.warning(f"Failed to extract listing {idx+1}: {e}")
        finally:
            await browser.close()
    return places

def save_places_to_csv(places: List[Place], output_path: str = "result.csv", append: bool = False):
    df = pd.DataFrame([asdict(place) for place in places])
    if not df.empty:
        # Drop columns that are entirely empty after all extractions
        df.dropna(axis=1, how='all', inplace=True)

        file_exists = os.path.isfile(output_path)
        mode = "a" if append else "w"
        header = not (append and file_exists)
        df.to_csv(output_path, index=False, mode=mode, header=header)
        logging.info(f"Saved {len(df)} places to {output_path} (append={append})")
    else:
        logging.warning("No data to save. DataFrame is empty.")

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", type=str, help="Search query for Google Maps")
    parser.add_argument("-t", "--total", type=int, help="Total number of results to scrape")
    parser.add_argument("-o", "--output", type=str, default="result.csv", help="Output CSV file path")
    parser.add_argument("--append", action="store_true", help="Append results to the output file instead of overwriting")
    args = parser.parse_args([]) # Pass empty list to prevent argparse from trying to parse sys.argv

    # Use kernel variables if they exist, otherwise use default/argparse defaults
    search_for = globals().get('SEARCH_KEYWORD', args.search or "turkish stores in toronto Canada")
    total = globals().get('MAX_PLACES', args.total or 1)
    output_path = args.output
    append = args.append

    places = await scrape_places(search_for, total)
    save_places_to_csv(places, output_path, append=append)

if __name__ == "__main__":
    # Run the async main function
    import nest_asyncio
    nest_asyncio.apply()
    asyncio.run(main())
# from google.colab import files
# files.download('result_test.csv')

"""**Reasoning**:
The previous errors indicated that `page.wait_for_selector` was timing out after clicking a listing, suggesting the detail page either takes longer to load or the selector is not immediately available. To address this, I will increase the timeout for waiting for the `name_xpath` to 30 seconds and add a `page.wait_for_loadstate('networkidle')` after clicking a listing to ensure the page has fully rendered before attempting to extract information.


"""

import logging
from typing import List, Optional
from playwright.async_api import async_playwright, Page
from dataclasses import dataclass, asdict
import pandas as pd
import argparse
import platform
import time
import os
import asyncio
import re

@dataclass
class Place:
    name: str = ""
    address: str = ""
    website: str = ""
    phone_number: str = ""
    reviews_count: Optional[int] = None
    reviews_average: Optional[float] = None
    store_shopping: str = "No"
    in_store_pickup: str = "No"
    store_delivery: str = "No"
    place_type: str = ""
    opens_at: str = ""
    introduction: str = ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

# XPaths defined globally to be accessible everywhere
name_xpath = '//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]'
address_xpath = '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'
website_xpath = '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'
phone_number_xpath = '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'
# Common parent for reviews info
reviews_info_parent_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]'
reviews_average_xpath = f'{reviews_info_parent_xpath}//span[@aria-hidden]'
# Revised reviews_count_xpath to be more robust, looking for span with "review" in aria-label or parentheses in text
reviews_count_xpath = f'{reviews_info_parent_xpath}//span[contains(@aria-label, "review")] | {reviews_info_parent_xpath}//span[contains(text(), "(") and contains(text(), ")")]'

info1 = '//div[@class="LTs0Rc"][1]'
info2 = '//div[@class="LTs0Rc"][2]'
info3 = '//div[@class="LTs0Rc"][3]'
opens_at_xpath = '//button[contains(@data-item-id, "oh")]//div[contains(@class, "fontBodyMedium")]'
opens_at_xpath2 = '//div[@class="MkV9"]//span[@class="ZDu9vd"]//span[2]'
place_type_xpath = '//div[@class="LBgpqf"]//button[@class="DkEaL "]'
intro_xpath = '//div[@class="WeS02d fontBodyMedium"]//div[@class="PYvSYb "]'

async def extract_text(page: Page, xpath: str) -> str:
    try:
        if await page.locator(xpath).count() > 0:
            return await page.locator(xpath).inner_text()
    except Exception as e:
        logging.warning(f"Failed to extract text for xpath {xpath}: {e}")
    return ""

async def extract_place(page: Page) -> Place:
    # XPaths are now globally defined, no need to redefine them here
    place = Place()
    place.name = await extract_text(page, name_xpath)
    place.address = await extract_text(page, address_xpath)
    place.website = await extract_text(page, website_xpath)
    place.phone_number = await extract_text(page, phone_number_xpath)
    place.place_type = await extract_text(page, place_type_xpath)
    place.introduction = await extract_text(page, intro_xpath) or "None Found"

    # Reviews Count
    reviews_count_raw = await extract_text(page, reviews_count_xpath)
    if reviews_count_raw:
        try:
            # Use regex to extract numbers, ignoring parentheses or "reviews" text
            temp = re.search(r'\d{1,3}(?:,\d{3})*', reviews_count_raw)
            if temp:
                place.reviews_count = int(temp.group(0).replace(',', ''))
            else:
                logging.warning(f"Failed to find number in reviews count string: '{reviews_count_raw}'")
        except Exception as e:
            logging.warning(f"Failed to parse reviews count from '{reviews_count_raw}': {e}")
    # Reviews Average
    reviews_avg_raw = await extract_text(page, reviews_average_xpath)
    if reviews_avg_raw:
        try:
            temp = reviews_avg_raw.replace(' ','').replace(',','.')
            place.reviews_average = float(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews average from '{reviews_avg_raw}': {e}")

    # Store Info
    for info_xpath in [info1, info2, info3]: # Removed idx as it was unused
        info_raw = await extract_text(page, info_xpath)
        if info_raw:
            temp = info_raw.split('·')
            if len(temp) > 1:
                check = temp[1].replace("\n", "").lower()
                if 'shop' in check:
                    place.store_shopping = "Yes"
                if 'pickup' in check:
                    place.in_store_pickup = "Yes"
                if 'delivery' in check:
                    place.store_delivery = "Yes"
    # Opens At
    opens_at_raw = await extract_text(page, opens_at_xpath)
    if opens_at_raw:
        opens = opens_at_raw.split('⋅')
        if len(opens) > 1:
            place.opens_at = opens[1].replace("\u202f","")
        else:
            place.opens_at = opens_at_raw.replace("\u202f","")
    else:
        opens_at2_raw = await extract_text(page, opens_at_xpath2)
        if opens_at2_raw:
            opens = opens_at2_raw.split('⋅')
            if len(opens) > 1:
                place.opens_at = opens[1].replace("\u202f","")
            else:
                place.opens_at = opens_at2_raw.replace("\u202f","")
    return place

async def scrape_places(search_for: str, total: int) -> List[Place]:
    setup_logging()
    places: List[Place] = []
    async with async_playwright() as p:
        if platform.system() == "Windows":
            browser_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            browser = await p.chromium.launch(executable_path=browser_path, headless=True)
        else:
            browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto("https://www.google.com/maps/@32.9817464,70.1930781,3.67z?", timeout=60000)
            await page.wait_for_timeout(1000)

            # Using get_by_role for more robust search box identification
            search_box = page.get_by_role("combobox", name="Search Google Maps")
            await search_box.wait_for(timeout=30000)
            await search_box.fill(search_for)
            await page.keyboard.press("Enter")

            await page.wait_for_timeout(5000)

            # Find the scrollable results pane
            scroll_pane_xpath = '//div[@role="feed"]'
            await page.wait_for_selector(scroll_pane_xpath, timeout=30000)
            scroll_pane = page.locator(scroll_pane_xpath)

            # Locator for individual result cards within the scroll pane
            listing_card_locator = scroll_pane.locator('div[role="article"]')

            previously_counted = 0
            # Scroll until 'total' items are found or no more items load
            while True:
                await scroll_pane.evaluate("node => node.scrollTop = node.scrollHeight")
                await page.wait_for_timeout(2000)

                current_found = await listing_card_locator.count()
                logging.info(f"Currently Found: {current_found}")

                if current_found >= total:
                    logging.info(f"Reached target total of {total} listings.")
                    break
                if current_found == previously_counted:
                    logging.info("Arrived at all available listings.")
                    break
                previously_counted = current_found

            # Get all the actual listing locators after scrolling
            all_listings = await listing_card_locator.all()
            listings_to_process = all_listings[:total]

            logging.info(f"Total Found for processing: {len(listings_to_process)}")

            for idx, listing in enumerate(listings_to_process):
                try:
                    await listing.click()
                    # Increased timeout for waiting for the name element on the detail page
                    await page.wait_for_selector(name_xpath, timeout=30000) # Increased timeout
                    await page.wait_for_loadstate('networkidle') # Wait for network to be idle
                    time.sleep(1.5)
                    place = await extract_place(page)
                    if place.name:
                        places.append(place)
                    else:
                        logging.warning(f"No name found for listing {idx+1} after click, skipping.")
                except Exception as e:
                    logging.warning(f"Failed to extract listing {idx+1}: {e}")
        finally:
            await browser.close()
    return places

def save_places_to_csv(places: List[Place], output_path: str = "result.csv", append: bool = False):
    df = pd.DataFrame([asdict(place) for place in places])
    if not df.empty:
        # Drop columns that are entirely empty after all extractions
        df.dropna(axis=1, how='all', inplace=True)

        file_exists = os.path.isfile(output_path)
        mode = "a" if append else "w"
        header = not (append and file_exists)
        df.to_csv(output_path, index=False, mode=mode, header=header)
        logging.info(f"Saved {len(df)} places to {output_path} (append={append})")
    else:
        logging.warning("No data to save. DataFrame is empty.")

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", type=str, help="Search query for Google Maps")
    parser.add_argument("-t", "--total", type=int, help="Total number of results to scrape")
    parser.add_argument("-o", "--output", type=str, default="result.csv", help="Output CSV file path")
    parser.add_argument("--append", action="store_true", help="Append results to the output file instead of overwriting")
    args = parser.parse_args([]) # Pass empty list to prevent argparse from trying to parse sys.argv

    # Use kernel variables if they exist, otherwise use default/argparse defaults
    search_for = globals().get('SEARCH_KEYWORD', args.search or "turkish stores in toronto Canada")
    total = globals().get('MAX_PLACES', args.total or 1)
    output_path = args.output
    append = args.append

    places = await scrape_places(search_for, total)
    save_places_to_csv(places, output_path, append=append)

if __name__ == "__main__":
    # Run the async main function
    import nest_asyncio
    nest_asyncio.apply()
    asyncio.run(main())
# from google.colab import files
# files.download('result_test.csv')

"""**Reasoning**:
The error "'Page' object has no attribute 'wait_for_loadstate'" indicates that the `wait_for_loadstate` method is not available or supported in the current Playwright environment. To fix this, I will remove the problematic `wait_for_loadstate` call and instead rely on an increased `time.sleep` duration to give the page more time to render after clicking a listing, in conjunction with the `wait_for_selector` for the element.


"""

import logging
from typing import List, Optional
from playwright.async_api import async_playwright, Page
from dataclasses import dataclass, asdict
import pandas as pd
import argparse
import platform
import time
import os
import asyncio
import re

@dataclass
class Place:
    name: str = ""
    address: str = ""
    website: str = ""
    phone_number: str = ""
    reviews_count: Optional[int] = None
    reviews_average: Optional[float] = None
    store_shopping: str = "No"
    in_store_pickup: str = "No"
    store_delivery: str = "No"
    place_type: str = ""
    opens_at: str = ""
    introduction: str = ""

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

# XPaths defined globally to be accessible everywhere
name_xpath = '//div[@class="TIHn2 "]//h1[@class="DUwDvf lfPIob"]'
address_xpath = '//button[@data-item-id="address"]//div[contains(@class, "fontBodyMedium")]'
website_xpath = '//a[@data-item-id="authority"]//div[contains(@class, "fontBodyMedium")]'
phone_number_xpath = '//button[contains(@data-item-id, "phone:tel:")]//div[contains(@class, "fontBodyMedium")]'
# Common parent for reviews info
reviews_info_parent_xpath = '//div[@class="TIHn2 "]//div[@class="fontBodyMedium dmRWX"]'
reviews_average_xpath = f'{reviews_info_parent_xpath}//span[@aria-hidden]'
# Revised reviews_count_xpath to be more robust, looking for span with "review" in aria-label or parentheses in text
reviews_count_xpath = f'{reviews_info_parent_xpath}//span[contains(@aria-label, "review")] | {reviews_info_parent_xpath}//span[contains(text(), "(") and contains(text(), ")")]'

info1 = '//div[@class="LTs0Rc"][1]'
info2 = '//div[@class="LTs0Rc"][2]'
info3 = '//div[@class="LTs0Rc"][3]'
opens_at_xpath = '//button[contains(@data-item-id, "oh")]//div[contains(@class, "fontBodyMedium")]'
opens_at_xpath2 = '//div[@class="MkV9"]//span[@class="ZDu9vd"]//span[2]'
place_type_xpath = '//div[@class="LBgpqf"]//button[@class="DkEaL "]'
intro_xpath = '//div[@class="WeS02d fontBodyMedium"]//div[@class="PYvSYb "]'

async def extract_text(page: Page, xpath: str) -> str:
    try:
        if await page.locator(xpath).count() > 0:
            return await page.locator(xpath).inner_text()
    except Exception as e:
        logging.warning(f"Failed to extract text for xpath {xpath}: {e}")
    return ""

async def extract_place(page: Page) -> Place:
    # XPaths are now globally defined, no need to redefine them here
    place = Place()
    place.name = await extract_text(page, name_xpath)
    place.address = await extract_text(page, address_xpath)
    place.website = await extract_text(page, website_xpath)
    place.phone_number = await extract_text(page, phone_number_xpath)
    place.place_type = await extract_text(page, place_type_xpath)
    place.introduction = await extract_text(page, intro_xpath) or "None Found"

    # Reviews Count
    reviews_count_raw = await extract_text(page, reviews_count_xpath)
    if reviews_count_raw:
        try:
            # Use regex to extract numbers, ignoring parentheses or "reviews" text
            temp = re.search(r'\d{1,3}(?:,\d{3})*', reviews_count_raw)
            if temp:
                place.reviews_count = int(temp.group(0).replace(',', ''))
            else:
                logging.warning(f"Failed to find number in reviews count string: '{reviews_count_raw}'")
        except Exception as e:
            logging.warning(f"Failed to parse reviews count from '{reviews_count_raw}': {e}")
    # Reviews Average
    reviews_avg_raw = await extract_text(page, reviews_average_xpath)
    if reviews_avg_raw:
        try:
            temp = reviews_avg_raw.replace(' ','').replace(',','.')
            place.reviews_average = float(temp)
        except Exception as e:
            logging.warning(f"Failed to parse reviews average from '{reviews_avg_raw}': {e}")

    # Store Info
    for info_xpath in [info1, info2, info3]: # Removed idx as it was unused
        info_raw = await extract_text(page, info_xpath)
        if info_raw:
            temp = info_raw.split('·')
            if len(temp) > 1:
                check = temp[1].replace("\n", "").lower()
                if 'shop' in check:
                    place.store_shopping = "Yes"
                if 'pickup' in check:
                    place.in_store_pickup = "Yes"
                if 'delivery' in check:
                    place.store_delivery = "Yes"
    # Opens At
    opens_at_raw = await extract_text(page, opens_at_xpath)
    if opens_at_raw:
        opens = opens_at_raw.split('⋅')
        if len(opens) > 1:
            place.opens_at = opens[1].replace("\u202f","")
        else:
            place.opens_at = opens_at_raw.replace("\u202f","")
    else:
        opens_at2_raw = await extract_text(page, opens_at_xpath2)
        if opens_at2_raw:
            opens = opens_at2_raw.split('⋅')
            if len(opens) > 1:
                place.opens_at = opens[1].replace("\u202f","")
            else:
                place.opens_at = opens_at2_raw.replace("\u202f","")
    return place

async def scrape_places(search_for: str, total: int) -> List[Place]:
    setup_logging()
    places: List[Place] = []
    async with async_playwright() as p:
        if platform.system() == "Windows":
            browser_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
            browser = await p.chromium.launch(executable_path=browser_path, headless=True)
        else:
            browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto("https://www.google.com/maps/@32.9817464,70.1930781,3.67z?", timeout=60000)
            await page.wait_for_timeout(1000)

            # Using get_by_role for more robust search box identification
            search_box = page.get_by_role("combobox", name="Search Google Maps")
            await search_box.wait_for(timeout=30000)
            await search_box.fill(search_for)
            await page.keyboard.press("Enter")

            await page.wait_for_timeout(5000)

            # Find the scrollable results pane
            scroll_pane_xpath = '//div[@role="feed"]'
            await page.wait_for_selector(scroll_pane_xpath, timeout=30000)
            scroll_pane = page.locator(scroll_pane_xpath)

            # Locator for individual result cards within the scroll pane
            listing_card_locator = scroll_pane.locator('div[role="article"]')

            previously_counted = 0
            # Scroll until 'total' items are found or no more items load
            while True:
                await scroll_pane.evaluate("node => node.scrollTop = node.scrollHeight")
                await page.wait_for_timeout(2000)

                current_found = await listing_card_locator.count()
                logging.info(f"Currently Found: {current_found}")

                if current_found >= total:
                    logging.info(f"Reached target total of {total} listings.")
                    break
                if current_found == previously_counted:
                    logging.info("Arrived at all available listings.")
                    break
                previously_counted = current_found

            # Get all the actual listing locators after scrolling
            all_listings = await listing_card_locator.all()
            listings_to_process = all_listings[:total]

            logging.info(f"Total Found for processing: {len(listings_to_process)}")

            for idx, listing in enumerate(listings_to_process):
                try:
                    await listing.click()
                    # Increased timeout for waiting for the name element on the detail page
                    await page.wait_for_selector(name_xpath, timeout=30000) # Increased timeout
                    # Removed page.wait_for_loadstate('networkidle')
                    time.sleep(2.5) # Increased sleep time for better stability
                    place = await extract_place(page)
                    if place.name:
                        places.append(place)
                    else:
                        logging.warning(f"No name found for listing {idx+1} after click, skipping.")
                except Exception as e:
                    logging.warning(f"Failed to extract listing {idx+1}: {e}")
        finally:
            await browser.close()
    return places

def save_places_to_csv(places: List[Place], output_path: str = "result.csv", append: bool = False):
    df = pd.DataFrame([asdict(place) for place in places])
    if not df.empty:
        # Drop columns that are entirely empty after all extractions
        df.dropna(axis=1, how='all', inplace=True)

        file_exists = os.path.isfile(output_path)
        mode = "a" if append else "w"
        header = not (append and file_exists)
        df.to_csv(output_path, index=False, mode=mode, header=header)
        logging.info(f"Saved {len(df)} places to {output_path} (append={append})")
    else:
        logging.warning("No data to save. DataFrame is empty.")

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--search", type=str, help="Search query for Google Maps")
    parser.add_argument("-t", "--total", type=int, help="Total number of results to scrape")
    parser.add_argument("-o", "--output", type=str, default="result.csv", help="Output CSV file path")
    parser.add_argument("--append", action="store_true", help="Append results to the output file instead of overwriting")
    args = parser.parse_args([]) # Pass empty list to prevent argparse from trying to parse sys.argv

    # Use kernel variables if they exist, otherwise use default/argparse defaults
    search_for = globals().get('SEARCH_KEYWORD', args.search or "turkish stores in toronto Canada")
    total = globals().get('MAX_PLACES', args.total or 1)
    output_path = args.output
    append = args.append

    places = await scrape_places(search_for, total)
    save_places_to_csv(places, output_path, append=append)

if __name__ == "__main__":
    # Run the async main function
    import nest_asyncio
    nest_asyncio.apply()
    asyncio.run(main())
# from google.colab import files
# files.download('result_test.csv')

"""**Reasoning**:
I have already fixed the scraping logic in the previous step by removing the problematic `wait_for_loadstate` and increasing the `time.sleep`. Now I need to execute the `main` function to perform the scraping with these fixes and generate the `result.csv` for analysis. I will then verify the scraped data.


"""

import asyncio
import nest_asyncio
import pandas as pd
import os

# Ensure async operations work in a notebook environment
nest_asyncio.apply()

# Use the variables from the kernel state for search query and total results
search_for = globals().get('SEARCH_KEYWORD', 'turkish stores in toronto Canada')
total_results = globals().get('MAX_PLACES', 1)
output_file = "result.csv"
append_mode = False # Overwrite previous (potentially empty) result.csv

print(f"Starting data scraping for '{search_for}' (total: {total_results})...")

try:
    # Call the async main function which uses the updated scrape_places and save_places_to_csv
    asyncio.run(main())

    print(f"Scraping completed. Data saved to '{output_file}'.")

    # Verify the content of the newly created CSV
    if os.path.exists(output_file):
        df_final = pd.read_csv(output_file)
        print("\nFirst 5 rows of the newly scraped DataFrame:")
        print(df_final.head())
        print("\nDataFrame Info after re-scraping:")
        df_final.info()
    else:
        print(f"Error: '{output_file}' was not created.")

except Exception as e:
    print(f"An unexpected error occurred during scraping: {e}")

"""## Data Cleaning and Preparation

### Subtask:
Clean the 'reviews_count' and 'reviews_average' columns by converting them to appropriate numeric types. Handle any missing values if necessary. Clean and standardize other categorical columns.

**Reasoning**:
I need to inspect the DataFrame `df_final` to verify the presence of specified columns, confirm data types for 'reviews_average', and standardize the 'place_type' column by converting it to lowercase and stripping whitespace, as well as checking the consistency of 'store_shopping', 'in_store_pickup', and 'store_delivery' columns.
"""

print("Columns in df_final:", df_final.columns.tolist())

# Check if 'reviews_count' is present (expected to be absent)
if 'reviews_count' in df_final.columns:
    print("'reviews_count' column found.")
else:
    print("'reviews_count' column is absent, as expected.")

# Verify data type of 'reviews_average'
print(f"Data type of 'reviews_average': {df_final['reviews_average'].dtype}")

# Examine unique values in 'place_type' before standardization
print("\nUnique 'place_type' values before cleaning:")
print(df_final['place_type'].unique())

# Standardize 'place_type' column
df_final['place_type'] = df_final['place_type'].str.lower().str.strip()
print("\nUnique 'place_type' values after cleaning:")
print(df_final['place_type'].unique())

# Inspect 'store_shopping', 'in_store_pickup', and 'store_delivery'
print("\nUnique 'store_shopping' values:")
print(df_final['store_shopping'].unique())
print("\nUnique 'in_store_pickup' values:")
print(df_final['in_store_pickup'].unique())
print("\nUnique 'store_delivery' values:")
print(df_final['store_delivery'].unique())

"""**Reasoning**:
Since the previous step confirmed 'reviews_count' is absent, 'reviews_average' is already numeric, and other categorical columns are standardized or consistent, I will check for any remaining missing values in 'reviews_average' and other relevant columns ('name', 'address', 'place_type', 'opens_at', 'introduction') to ensure data quality before proceeding. If missing values are found, I will decide on an appropriate handling strategy (e.g., imputation or removal, though given the nature of the data, removal or specific handling per column is more likely).


"""

print("\nMissing values before handling:")
print(df_final[['name', 'address', 'website', 'phone_number', 'reviews_average', 'place_type', 'opens_at', 'introduction']].isnull().sum())

# For 'reviews_average', since it's numeric, we can fill NaN with its mean or median if appropriate, or 0 if no reviews imply 0 average.
# Given it's an average, NaN likely means no reviews, so filling with 0 or the mean might be misleading if reviews_count is not available.
# Since reviews_count is absent, we'll keep reviews_average as is for now, as it's already a float.
# If there are missing 'reviews_average' values, filling them with the mean or median could be an option,
# but let's assume for now that missing values imply no average could be calculated (e.g., no reviews).
# For 'opens_at', we can fill missing values with 'Not specified' or 'Always open' if that makes sense semantically.
# For now, let's just inspect them.

# Display some rows with missing 'reviews_average' or 'opens_at' if any
print("\nRows with missing 'reviews_average':")
print(df_final[df_final['reviews_average'].isnull()].head())

print("\nRows with missing 'opens_at':")
print(df_final[df_final['opens_at'].isnull()].head())

# Given the current data, 'reviews_average' has no missing values.
# 'opens_at' has some missing values, which can be handled if needed, for now we will just keep them.

"""**Reasoning**:
The previous steps confirmed that 'reviews_count' is absent, 'reviews_average' is already numeric and has no missing values. The 'place_type' column has been standardized. I will now address the missing values in 'website', 'phone_number', and 'opens_at' columns by filling them with appropriate placeholder strings to ensure data completeness for further analysis.


"""

df_final['website'] = df_final['website'].fillna('Not available')
df_final['phone_number'] = df_final['phone_number'].fillna('Not available')
df_final['opens_at'] = df_final['opens_at'].fillna('Not specified')

print("\nMissing values after handling:")
print(df_final[['website', 'phone_number', 'opens_at']].isnull().sum())

print("\nFirst 5 rows after handling missing values (showing relevant columns):")
print(df_final[['name', 'website', 'phone_number', 'opens_at']].head())

"""## Analyze Place Types

### Subtask:
Analyze the distribution of different 'place_type' categories. Count the occurrences of each type and identify the most common ones.

"""

from google.colab import files
files.download('result.csv')

"""**Reasoning**:
To analyze the distribution of 'place_type' categories, I will calculate the frequency of each unique place type using `value_counts()` and then print the result.


"""

print("\nDistribution of 'place_type' categories:")
place_type_distribution = df_final['place_type'].value_counts()
print(place_type_distribution)

"""**Reasoning**:
To visualize the distribution of 'place_type' categories, I will create a bar chart using matplotlib and seaborn to show the counts of each place type, making it easier to interpret the most common types.


"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x=place_type_distribution.index, y=place_type_distribution.values, palette='viridis')
plt.title('Distribution of Place Types')
plt.xlabel('Place Type')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**Reasoning**:
The previous plot generated a `FutureWarning` from seaborn regarding the usage of `palette` without `hue` in `sns.barplot`. To address this and follow best practices for future versions, I will modify the `sns.barplot` call to explicitly assign the `x` variable to the `hue` parameter and set `legend=False`.


"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.barplot(x=place_type_distribution.index, y=place_type_distribution.values, hue=place_type_distribution.index, palette='viridis', legend=False)
plt.title('Distribution of Place Types')
plt.xlabel('Place Type')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""## Analyze Review Data

### Subtask:
Calculate descriptive statistics (mean, median, min, max) for 'reviews_average'. Identify places with high average ratings.

**Reasoning**:
To calculate descriptive statistics for 'reviews_average' and identify places with high average ratings, I will first use the `.describe()` method on the 'reviews_average' column to get the mean, median, min, and max. Then, I will sort the DataFrame by 'reviews_average' in descending order and select the top 5 records.
"""

print("\nDescriptive statistics for 'reviews_average':")
print(df_final['reviews_average'].describe())

print("\nTop 5 places with the highest 'reviews_average':")
top_rated_places = df_final.sort_values(by='reviews_average', ascending=False).head(5)
print(top_rated_places[['name', 'reviews_average', 'place_type']])

"""## Analyze Store Services

### Subtask:
Examine the distribution of 'store_shopping', 'in_store_pickup', and 'store_delivery' services to understand common offerings.

**Reasoning**:
To understand the distribution of store services, I will print the value counts for 'store_shopping', 'in_store_pickup', and 'store_delivery' columns.
"""

print("\nDistribution of 'store_shopping' service:")
print(df_final['store_shopping'].value_counts())

print("\nDistribution of 'in_store_pickup' service:")
print(df_final['in_store_pickup'].value_counts())

print("\nDistribution of 'store_delivery' service:")
print(df_final['store_delivery'].value_counts())

"""### Summary of Store Service Availability

Based on the analysis of the `df_final` DataFrame, it appears that none of the scraped places (all 50 entries) offer 'store_shopping', 'in_store_pickup', or 'store_delivery' services. All three columns ('store_shopping', 'in_store_pickup', and 'store_delivery') exclusively show 'No' as their value, indicating a complete absence of these specific services among the retrieved listings for 'Bar in Dhaka'.

## Visualize Key Metrics

### Subtask:
Create visualizations such as bar charts for place types and store services, and a histogram for reviews average, with appropriate legends. This will help in understanding the data distribution visually.

**Reasoning**:
To visualize the distribution of 'reviews_average', I will create a histogram using matplotlib and seaborn, setting an appropriate number of bins and adding relevant labels and a title.
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df_final['reviews_average'], bins=10, kde=True)
plt.title('Distribution of Average Review Scores')
plt.xlabel('Average Review Score')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.tight_layout()
plt.show()

"""**Reasoning**:
To visualize the distribution of store services, I will create bar charts for 'store_shopping', 'in_store_pickup', and 'store_delivery' services using seaborn to show their counts.


"""

plt.figure(figsize=(18, 6))

plt.subplot(1, 3, 1)
sns.barplot(x=df_final['store_shopping'].value_counts().index, y=df_final['store_shopping'].value_counts().values, palette='viridis', hue=df_final['store_shopping'].value_counts().index, legend=False)
plt.title('Distribution of Store Shopping Service')
plt.xlabel('Store Shopping Available')
plt.ylabel('Count')

plt.subplot(1, 3, 2)
sns.barplot(x=df_final['in_store_pickup'].value_counts().index, y=df_final['in_store_pickup'].value_counts().values, palette='viridis', hue=df_final['in_store_pickup'].value_counts().index, legend=False)
plt.title('Distribution of In-Store Pickup Service')
plt.xlabel('In-Store Pickup Available')
plt.ylabel('Count')

plt.subplot(1, 3, 3)
sns.barplot(x=df_final['store_delivery'].value_counts().index, y=df_final['store_delivery'].value_counts().values, palette='viridis', hue=df_final['store_delivery'].value_counts().index, legend=False)
plt.title('Distribution of Store Delivery Service')
plt.xlabel('Store Delivery Available')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""## Final Task

### Subtask:
Provide a comprehensive summary of the analysis, including key findings about place types, review statistics, and store services. Suggest further steps for deeper analysis or insights.

## Summary:

### Data Analysis Key Findings

*   **Initial Data Quality:** The initial `result.csv` was empty, containing only a `reviews_count` column with 0 non-null values, leading to a necessary re-scraping process.
*   **Successful Scraping:** The re-scraping successfully collected data for 50 places, populating fields such as name, address, website, phone number, reviews average, place type, and operating hours. However, the `reviews_count` column remained unpopulated in the final dataset.
*   **Data Cleaning and Preparation:**
    *   The `reviews_average` column was successfully identified as `float64` with no missing values.
    *   The `place_type` column was standardized to lowercase and stripped of whitespace, resulting in `['bar', 'lounge', 'bar & grill']` as unique categories.
    *   Missing values in `website` (40 entries), `phone_number` (6 entries), and `opens_at` (6 entries) were filled with 'Not available' or 'Not specified'.
*   **Place Type Distribution:** The most common place type was 'bar' with 42 occurrences, followed by 'lounge' (6 occurrences) and 'bar & grill' (2 occurrences).
*   **Review Statistics:**
    *   The average review score (`reviews_average`) across all 50 places was approximately 4.28.
    *   The minimum average review score was 3.7, and the maximum was 5.0.
    *   The median average review score was 4.3.
    *   Three places achieved a perfect 5.0 rating: `A.S.R Pizza & Momo Bar`, `Shardul Bar at Holiday Inn Dhaka City Centre`, and `Zakaria Restaurant and Bar`.
*   **Store Services:** None of the 50 scraped places offered 'store_shopping', 'in_store_pickup', or 'store_delivery' services. All entries for these columns were consistently marked as 'No'.

### Insights or Next Steps

*   **Improve Scraping Robustness:** Investigate why the `reviews_count` column failed to populate during scraping, as this metric is crucial for understanding the reliability of average review scores (e.g., a 5.0 rating based on 1 review is different from one based on 100 reviews).
*   **Expand Service Analysis:** The current data suggests no services like shopping, pickup, or delivery are offered by these types of establishments. This could be due to the specific search query ("Bar in Dhaka") or a limitation of the scraping process. Future analysis could include a wider range of business types or verify service availability through other means.
"""